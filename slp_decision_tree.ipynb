{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-mnist will install the required package\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from threading import Thread\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision                                 # datasets and transformations modules\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn                              # neural network module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim                        # optimization module\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# All networks derive from the base class nn.Module\n",
    "class Perceptron(nn.Module):\n",
    "    # get input and output dimensions as input\n",
    "    def __init__(self, d, K):\n",
    "        # all derived classes must call __init__ method of super class\n",
    "        super(Perceptron, self).__init__()\n",
    "        # create a fully connected layer from input to output\n",
    "        self.model = nn.Linear(d, K)\n",
    "    \n",
    "    # forward method should get the input and return the output\n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        # flatten the image from BxCxHXW to Bx784\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.model(x.float())\n",
    "        # softmax is internally done inside cross entropy loss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlpGeneticForestClassifier:\n",
    "    def __init__(self, N, generation_number, class_percentage):\n",
    "        self.N = N\n",
    "        self.generation_number = generation_number\n",
    "        self.class_percentage = class_percentage\n",
    "        self.trained_trees = []\n",
    "        self.slp = None\n",
    "        self.prediction_tree = None\n",
    "        self.one_hot_encoder = None\n",
    "        # torch parameters\n",
    "        self.SEED = 0            # reproducability\n",
    "        # NN Parameters\n",
    "        self.EPOCHS = 20          # number of epochs\n",
    "        self.LR = 0.01            # learning rate\n",
    "        self.MOMENTUM = 0.9       # momentum for the optimizer\n",
    "        self.WEIGHT_DECAY = 1e-5  # weight decay for the optimizer\n",
    "        self.GAMMA = 0.1          # learning rate schedular\n",
    "        self.BATCH_SIZE = 32      # number of images to load per iteration\n",
    "\n",
    "    def train_net(self):\n",
    "        # put the network in training mode\n",
    "        self.slp.train()\n",
    "        # keep record of the loss value\n",
    "        epoch_loss = 0.0\n",
    "        # use training data as batches\n",
    "        for xt, rt in self.train_loader:\n",
    "            # move training instances and corresponding labels into gpu if cuda is available\n",
    "            xt, rt = xt.to(self.device), rt.to(self.device)\n",
    "            # clear the previously accumulated gradients\n",
    "            self.optimizer.zero_grad() \n",
    "            # forward the network\n",
    "            yt = self.slp(xt)\n",
    "            # calculate loss\n",
    "            loss = self.loss_fn(yt, rt)\n",
    "            # make a backward pass, calculate gradients\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            self.optimizer.step()\n",
    "            # accumulate loss\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss\n",
    "        \n",
    "    def train_tree(self, filter_label, state_counter, tree_list):\n",
    "        \n",
    "        y_train_subtree = self.y_train[self.y_train.label.isin(filter_label)]\n",
    "        X_train_subtree = self.X_train.loc[y_train_subtree.index.values.tolist()]\n",
    "        \n",
    "        dtc = DecisionTreeClassifier(random_state=state_counter)\n",
    "        dtc = dtc.fit(X_train_subtree,y_train_subtree)\n",
    "        y_valid_filtered= self.y_valid[self.y_valid.label.isin(filter_label)]\n",
    "        X_valid_filtered = self.X_valid.loc[y_valid_filtered.index]\n",
    "        y_pred = dtc.predict(X_valid_filtered)\n",
    "        tree_accuracy = metrics.accuracy_score(y_valid_filtered, y_pred)\n",
    "        print(\"Accuracy of Tree\",state_counter+1,\":\",tree_accuracy)\n",
    "        print(\"Classes: \",filter_label)\n",
    "        tree_list.append({\"tree\": dtc, \"accuracy\": tree_accuracy, \"filter_label\": filter_label})   \n",
    "        \n",
    "    def fit_trees(self, filter_labels, tree_list, thread_batch):\n",
    "        state_counter = 0\n",
    "        train_threads = []\n",
    "        for filter_label in filter_labels:\n",
    "            train_threads.append(Thread(target=self.train_tree, args=[filter_label, state_counter, tree_list]))\n",
    "            state_counter += 1\n",
    "        for thread_index in range(0, len(train_threads), thread_batch):\n",
    "            current_train_threads = train_threads[thread_index:thread_index+thread_batch]\n",
    "            for train_thread in current_train_threads:\n",
    "                train_thread.start()\n",
    "            for train_thread in current_train_threads:\n",
    "                train_thread.join()\n",
    "                \n",
    "    def train_slp(self, one_hot_encoded_predictions):\n",
    "        self.d = one_hot_encoded_predictions.shape[1]      # number of input features \n",
    "        \n",
    "        print(\"SLP input dimension:\", self.d)\n",
    "        \n",
    "        # manual seed to reproduce same resultsnet\n",
    "        torch.manual_seed(self.SEED)\n",
    "        # create the network\n",
    "        self.slp = Perceptron(self.d,self.K)\n",
    "        # check if CUDA is available\n",
    "        cuda = torch.cuda.is_available()  \n",
    "        self.device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "        # if cuda is available move network into gpu\n",
    "        self.slp.to(self.device)\n",
    "        # specify the loss to be used\n",
    "        # softmax is internally computed.\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        # specify the optimizer to update the weights during backward pass\n",
    "        self.optimizer = optim.SGD(self.slp.parameters(), lr=self.LR, momentum=self.MOMENTUM, weight_decay=self.WEIGHT_DECAY)\n",
    "        # change learning rate over time\n",
    "        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=self.GAMMA) #CHECK THIS\n",
    "        \n",
    "        train_target = torch.tensor(self.y_train.values.flatten().astype(np.int32)).long()\n",
    "\n",
    "        train = torch.tensor(one_hot_encoded_predictions) \n",
    "\n",
    "        train_tensor = torch.utils.data.TensorDataset(train, train_target) \n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = self.BATCH_SIZE, shuffle = True, num_workers=8)\n",
    "        \n",
    "        # train the network\n",
    "        for epoch in range(1,self.EPOCHS+1):\n",
    "            # train network for one epoch\n",
    "            self.train_net()\n",
    "        # print(\"SLP Weights:\", self.slp.model.weight)\n",
    "        \n",
    "        \n",
    "    def one_hot_encode(self):\n",
    "        total_predictions = self.trained_trees[0][\"tree\"].predict(self.X_train)\n",
    "        for i in range(1, self.N):\n",
    "            total_predictions = np.vstack([total_predictions, self.trained_trees[i][\"tree\"].predict(self.X_train)])\n",
    "        total_predictions = np.transpose(total_predictions)\n",
    "        self.one_hot_encoder = OneHotEncoder(handle_unknown='ignore') \n",
    "        self.one_hot_encoder.fit(total_predictions)\n",
    "        \n",
    "        one_hot_encoded_predictions = self.one_hot_encoder.transform(total_predictions).toarray() \n",
    "        return one_hot_encoded_predictions\n",
    "\n",
    "    \n",
    "    def train_prediction_tree(self, one_hot_encoded_predictions):\n",
    "        self.prediction_tree = DecisionTreeClassifier(random_state=200)\n",
    "        self.prediction_tree = self.prediction_tree.fit(one_hot_encoded_predictions, self.y_train)\n",
    "        \n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)  # Train-test split pairs\n",
    "        self.label_count = len(y_train.label.unique())\n",
    "        self.sample_count = y_train.shape[0]\n",
    "        self.K = self.label_count              # number of output features\n",
    "        \n",
    "        filter_labels = self.genetic_find_parameters()\n",
    "        self.fit_trees(filter_labels, self.trained_trees, thread_batch=20)\n",
    "        \n",
    "        one_hot_encoded_predictions = self.one_hot_encode()\n",
    "        \n",
    "        self.train_slp(one_hot_encoded_predictions)\n",
    "        \n",
    "        self.train_prediction_tree(one_hot_encoded_predictions)\n",
    "        \n",
    "    \n",
    "    def majority_voting_predict(self, X_test):\n",
    "        total_predictions = self.forest_trees_predict(X_test)\n",
    "        # Majority Voting\n",
    "        predicted_values = []\n",
    "        for row in total_predictions:\n",
    "            majority_vote = np.bincount(row).argmax()\n",
    "            predicted_values.append(majority_vote)\n",
    "        y_pred_class = np.asarray(predicted_values)\n",
    "        return y_pred_class\n",
    "    \n",
    "    def slp_predict(self, X_test):\n",
    "        total_predictions = self.forest_trees_predict(X_test)\n",
    "        # SLP\n",
    "        one_hot_encoded_predictions = self.one_hot_encoder.transform(total_predictions).toarray() \n",
    "        test = torch.tensor(one_hot_encoded_predictions) \n",
    "        y_pred = self.slp(test.to(self.device))\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "        y_pred_class = np.asarray([np.argmax(pred) for pred in y_pred])\n",
    "        return y_pred_class\n",
    "    \n",
    "    def prediction_tree_predict(self, X_test):\n",
    "        total_predictions = self.forest_trees_predict(X_test)\n",
    "        # Prediction Tree\n",
    "        one_hot_encoded_predictions = self.one_hot_encoder.transform(total_predictions).toarray() \n",
    "        y_pred_class = self.prediction_tree.predict(one_hot_encoded_predictions)\n",
    "        return y_pred_class\n",
    "    \n",
    "    def forest_trees_predict(self, X_test):\n",
    "        total_predictions = self.trained_trees[0][\"tree\"].predict(X_test)\n",
    "        for i in range(1, self.N):\n",
    "            total_predictions = np.vstack([total_predictions, self.trained_trees[i][\"tree\"].predict(X_test)])\n",
    "        total_predictions = np.transpose(total_predictions)\n",
    "        return total_predictions\n",
    "    \n",
    "    \n",
    "    # Genetic algorithm  \n",
    "    def generate_parent_samples(self):\n",
    "        generation = []\n",
    "        for i in range(self.N):\n",
    "            generation.append(\n",
    "                #{\"max_depth\":  np.random.normal(np.log2(self.sample_count)*2, np.log2(self.sample_count), 1),\n",
    "                #\"min_samples_split\": np.random.randint(2,self.label_count),\n",
    "                #\"min_samples_leaf\": np.random.randint(2,self.label_count),\n",
    "                #\"max_leaf_nodes\": np.random.randint(10, self.sample_count),}\n",
    "                np.random.choice(range(self.label_count), round(self.label_count*self.class_percentage), replace=False)\n",
    "            )\n",
    "        return generation  \n",
    "\n",
    "\n",
    "    def genetic_find_parameters(self):\n",
    "        generation = self.generate_parent_samples()\n",
    "        #print(\"Generation\\n\",\"*\"*50)\n",
    "        #print(generation)\n",
    "        #print(\"*\"*50)\n",
    "        for i in range(self.generation_number):\n",
    "            generation = self.evolve(generation)\n",
    "            print(\"Gen:\",i+1)\n",
    "        return generation\n",
    "    \n",
    "        \n",
    "    def evolve(self, generation):\n",
    "        trained_tree_results = []\n",
    "        self.fit_trees(generation, trained_tree_results, thread_batch=25)\n",
    "        trained_tree_results_sorted = sorted(trained_tree_results, key=itemgetter(\"accuracy\"), reverse=True)\n",
    "        \n",
    "        next_generation = []\n",
    "        next_generation.append(trained_tree_results_sorted[0][\"filter_label\"])\n",
    "    \n",
    "        for i in range(1, len(generation)):\n",
    "            parent_1 = self.tournament(trained_tree_results)\n",
    "            parent_2 = self.tournament(trained_tree_results)\n",
    "            child = self.crossover(parent_1, parent_2)\n",
    "            # self.mutate(child)\n",
    "            next_generation.append(child)\n",
    "            \n",
    "        return next_generation\n",
    "\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        parents_merged = np.unique(np.append(parent1, parent2))\n",
    "        child = np.random.choice(parents_merged, len(parent1), replace=False)\n",
    "        return child\n",
    "\n",
    "    \n",
    "    def mutate(self, child):\n",
    "        return\n",
    "\n",
    "    \n",
    "    def tournament(self, generation):\n",
    "        # print(\"*********** Tournament ***********\")\n",
    "        accuracies = np.asarray([tree[\"accuracy\"] for tree in generation])\n",
    "        accuracies -= np.min(accuracies)\n",
    "        probabilities = np.asarray(accuracies)/sum(accuracies)\n",
    "        # print(\"Probabilities:\",probabilities)\n",
    "        selected = np.random.choice(generation, 1, p=probabilities)[0][\"filter_label\"]\n",
    "        # print(\"Selected:\", selected)\n",
    "        return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(60) # reproducability\n",
    "\n",
    "def mnist_dataset_read(path):\n",
    "    mndata = MNIST(path)\n",
    "\n",
    "    # read training images and corresponding labels\n",
    "    tr_images, tr_labels = mndata.load_training()\n",
    "    # read test images and corresponding labels\n",
    "    tt_images, tt_labels = mndata.load_testing()\n",
    "\n",
    "    # convert lists into numpy format and apply normalization\n",
    "    tr_images = np.array(tr_images) / 255. # shape (60000, 784)\n",
    "    tr_labels = np.array(tr_labels)         # shape (60000,)\n",
    "    tt_images = np.array(tt_images) / 255. # shape (10000, 784)\n",
    "    tt_labels = np.array(tt_labels)         # shape (10000,)\n",
    "\n",
    "    columns_images = ['p{}'.format(i+1) for i in range(784)]\n",
    "    X_train = pd.DataFrame(data=tr_images, columns=columns_images)\n",
    "    y_train = pd.DataFrame(data=tr_labels, columns=['label'])\n",
    "    X_test = pd.DataFrame(data=tt_images, columns=columns_images)\n",
    "    y_test = pd.DataFrame(data=tt_labels, columns=['label'])\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_data(btch):\n",
    "    labels = btch[b'labels']\n",
    "    imgs = btch[b'data'].reshape((-1, 32, 32, 3))\n",
    "    \n",
    "    res = []\n",
    "    for ii in range(imgs.shape[0]):\n",
    "        img = imgs[ii].copy()\n",
    "        #img = np.transpose(img.flatten().reshape(3,32,32))\n",
    "        img = np.fliplr(np.rot90(np.transpose(img.flatten().reshape(3,32,32)), k=-1))\n",
    "        res.append(img)\n",
    "    imgs = np.stack(res)\n",
    "    return labels, imgs\n",
    "\n",
    "def load_data_cifar():\n",
    "    batch1 = unpickle(\"Datasets/cifar-10-batches-py/data_batch_1\")\n",
    "    batch2 = unpickle(\"Datasets/cifar-10-batches-py/data_batch_2\")\n",
    "    batch3 = unpickle(\"Datasets/cifar-10-batches-py/data_batch_3\")\n",
    "    batch4 = unpickle(\"Datasets/cifar-10-batches-py/data_batch_4\")\n",
    "    batch5 = unpickle(\"Datasets/cifar-10-batches-py/data_batch_5\")\n",
    "    test_batch = unpickle(\"Datasets/cifar-10-batches-py/test_batch\")\n",
    "    \n",
    "    pixel_num = 32*32*3\n",
    "    x_train_l = []\n",
    "    y_train_l = []\n",
    "    for ibatch in [batch1, batch2, batch3, batch4, batch5]:\n",
    "        labels, imgs = load_data(ibatch)\n",
    "        x_train_l.append(imgs)\n",
    "        y_train_l.extend(labels)\n",
    "    x_train = np.vstack(x_train_l)\n",
    "    y_train = np.vstack(y_train_l)\n",
    "    \n",
    "    x_test_l = []\n",
    "    y_test_l = []\n",
    "    labels, imgs = load_data(test_batch)\n",
    "    x_test_l.append(imgs)\n",
    "    y_test_l.extend(labels)\n",
    "    x_test = np.vstack(x_test_l)\n",
    "    y_test = np.vstack(y_test_l)\n",
    "    \n",
    "    del batch1, batch2, batch3, batch4, batch5, test_batch\n",
    "    \n",
    "    # imgplot = plt.imshow(x_train[58])\n",
    "    \n",
    "    x_train, x_test = x_train.reshape(-1, pixel_num), x_test.reshape(-1, pixel_num)\n",
    "    \n",
    "    columns_images = ['p{}'.format(i+1) for i in range(pixel_num)]\n",
    "    X_train = pd.DataFrame(data=x_train, columns=columns_images)\n",
    "    y_train = pd.DataFrame(data=y_train, columns=['label'])\n",
    "    X_test = pd.DataFrame(data=x_test, columns=columns_images)\n",
    "    y_test = pd.DataFrame(data=y_test, columns=['label'])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(X_train, X_test, y_train, y_test):\n",
    "    sgfc = SlpGeneticForestClassifier(N=50, generation_number=0, class_percentage = 0.5)\n",
    "    sgfc.fit(X_train, y_train)\n",
    "\n",
    "    slp_pred=sgfc.slp_predict(X_test)\n",
    "    prediction_tree_pred=sgfc.prediction_tree_predict(X_test)\n",
    "    majority_voting_pred=sgfc.majority_voting_predict(X_test)\n",
    "\n",
    "    print(\"SLP Pred: \", metrics.accuracy_score(y_test, slp_pred))\n",
    "    print(\"Prediction Tree Pred: \", metrics.accuracy_score(y_test, prediction_tree_pred))\n",
    "    print(\"Majority Voting Pred: \", metrics.accuracy_score(y_test, majority_voting_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(n_estimators=100)\n",
    "xX_train, xX_valid, yy_train, yy_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)  # Train-test split pairs\n",
    "rf.fit(xX_train,yy_train)\n",
    "pred=rf.predict(xX_valid)\n",
    "print(metrics.accuracy_score(yy_valid, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_X_train, m_X_test, m_y_train, m_y_test = mnist_dataset_read('Datasets/MNIST')\n",
    "fm_X_train, fm_X_test, fm_y_train, fm_y_test = mnist_dataset_read('Datasets/Fashion_MNIST')\n",
    "cf_X_train, cf_y_train, cf_X_test, cf_y_test = load_data_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n",
      "\n",
      "Accuracy of Tree 19 : 0.9085914085914086\n",
      "Classes:  [1 3 5 8 4]\n",
      "Accuracy of Tree 15 : 0.934199566883225\n",
      "Classes:  [0 4 5 8 1]\n",
      "Accuracy of Tree 16 : 0.9400102546573236\n",
      "Classes:  [6 5 7 4 0]\n",
      "Accuracy of Tree 20 : 0.9413621262458471\n",
      "Classes:  [1 4 3 5 0]\n",
      "Accuracy of Tree 8 : 0.9034334763948498\n",
      "Classes:  [9 5 1 3 8]\n",
      "Accuracy of Tree 7 : 0.933474645361161\n",
      "Classes:  [4 0 9 1 8]\n",
      "Accuracy of Tree 3 : 0.9184373978424322\n",
      "Classes:  [1 9 6 8 4]\n",
      "Accuracy of Tree 11 : 0.9253274366388842\n",
      "Classes:  [5 0 2 7 9]\n",
      "Accuracy of Tree 4 : 0.923340401239602\n",
      "Classes:  [7 9 1 4 2]\n",
      "Accuracy of Tree 1 : 0.8955912508544087\n",
      "Classes:  [5 8 7 4 9]\n",
      "Accuracy of Tree 12 : 0.9117647058823529\n",
      "Classes:  [7 3 5 0 2]\n",
      "Accuracy of Tree 9 : 0.9328668998833917\n",
      "Classes:  [4 8 0 5 1]\n",
      "Accuracy of Tree 17 : 0.9139566395663956\n",
      "Classes:  [5 7 0 9 8]\n",
      "Accuracy of Tree 5 : 0.9051337431467021\n",
      "Classes:  [3 1 5 8 2]\n",
      "Accuracy of Tree 2 : 0.9471472442217553\n",
      "Classes:  [1 3 9 6 0]\n",
      "Accuracy of Tree 10 : 0.9021870156707422\n",
      "Classes:  [3 4 2 5 9]\n",
      "Accuracy of Tree 14 : 0.9083503054989817\n",
      "Classes:  [8 5 7 6 3]\n",
      "Accuracy of Tree 13 : 0.9165551839464883\n",
      "Classes:  [7 2 9 8 6]\n",
      "Accuracy of Tree 18 : 0.955877616747182\n",
      "Classes:  [0 1 7 3 6]\n",
      "Accuracy of Tree 6 : 0.9037900874635568\n",
      "Classes:  [3 5 4 6 9]\n",
      "Accuracy of TreeAccuracy of Tree 29 : 0.9205976162497902\n",
      "Classes:  [2 9 8 0 6]\n",
      " 33 : 0.9464959789922862\n",
      "Classes:  [2 6 1 4 0]\n",
      "Accuracy of Tree 22 : 0.906538856555974\n",
      "Classes:  [0 5 8 9 2]\n",
      "Accuracy of Tree 21 : 0.9403654485049834\n",
      "Classes:  [0 4 5 1 3]\n",
      "Accuracy of Tree 26 : 0.9357305564594859\n",
      "Classes:  [0 2 8 9 1]\n",
      "Accuracy of Tree 30 : 0.9178035470668485\n",
      "Classes:  [6 7 2 5 9]\n",
      "Accuracy of Tree 34 : 0.9074136763710223\n",
      "Classes:  [3 4 2 6 8]\n",
      "Accuracy of Tree 25 : 0.9449496589801883\n",
      "Classes:  [1 7 3 6 4]\n",
      "Accuracy of Tree 37 : 0.9302631578947368\n",
      "Classes:  [3 6 7 5 1]\n",
      "Accuracy of Tree 38 : 0.9297982324495581\n",
      "Classes:  [6 2 3 7 9]\n",
      "Accuracy of Tree 40 : 0.9302516861325876\n",
      "Classes:  [2 6 4 1 8]\n",
      "Accuracy of Tree 32 : 0.9241619648191172\n",
      "Classes:  [4 8 7 1 5]\n",
      "Accuracy of Tree 36 : 0.9338025808614044\n",
      "Classes:  [8 4 0 6 7]\n",
      "Accuracy of Tree 28 : 0.9298558237485826\n",
      "Classes:  [9 1 7 4 3]\n",
      "Accuracy of Tree 27 : 0.8999669858038957\n",
      "Classes:  [3 1 8 5 9]\n",
      "Accuracy of Tree 23 : 0.919882839421089\n",
      "Classes:  [4 5 2 9 0]\n",
      "Accuracy of Tree 39 : 0.9294407894736842\n",
      "Classes:  [6 5 7 1 3]\n",
      "Accuracy of Tree 24 : 0.9215750615258409\n",
      "Classes:  [9 7 3 1 5]\n",
      "Accuracy of Tree 31 : 0.937798255718282\n",
      "Classes:  [1 5 7 6 9]\n",
      "Accuracy of Tree 35 : 0.9236539423237207\n",
      "Classes:  [8 0 6 9 3]\n",
      "Accuracy of Tree 47 : 0.9165721691236027\n",
      "Classes:  [1 8 6 9 3]\n",
      "Accuracy of Tree 43 : 0.8963446027115154\n",
      "Classes:  [2 8 9 6 5]\n",
      "Accuracy of Tree 44 : 0.9209187858900738\n",
      "Classes:  [9 5 1 7 3]\n",
      "Accuracy of Tree 48 : 0.9005524861878453\n",
      "Classes:  [4 2 6 5 3]\n",
      "Accuracy of Tree 41 : 0.9160033869602032\n",
      "Classes:  [6 8 4 0 2]\n",
      "Accuracy of Tree 49 : 0.9424118129614438\n",
      "Classes:  [3 0 5 1 7]\n",
      "Accuracy of Tree 42 : 0.916083916083916\n",
      "Classes:  [8 1 3 2 9]\n",
      "Accuracy of Tree 46 : 0.9247239879558381\n",
      "Classes:  [5 1 4 9 2]\n",
      "Accuracy of Tree 50 : 0.9162465441535209\n",
      "Classes:  [1 8 2 9 3]\n",
      "Accuracy of Tree 45 : 0.9262964815741204\n",
      "Classes:  [7 9 2 6 3]\n",
      "SLP input dimension: 250\n",
      "SLP Pred:  0.9626\n",
      "Prediction Tree Pred:  0.8493\n",
      "Majority Voting Pred:  0.9573\n"
     ]
    }
   ],
   "source": [
    "# MNIST\n",
    "print(\"MNIST\\n\")\n",
    "run_test(m_X_train, m_X_test, m_y_train, m_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion MNIST\n",
      "\n",
      "Accuracy of Tree 6 : 0.9147765176784523\n",
      "Classes:  [3 4 7 9 1]\n",
      "Accuracy of Tree 9 : 0.8367622259696459\n",
      "Classes:  [0 4 7 1 6]\n",
      "Accuracy of Tree 15 : 0.9328703703703703\n",
      "Classes:  [1 8 9 4 3]\n",
      "Accuracy of Tree 10 : 0.9426312247644684\n",
      "Classes:  [7 1 0 2 9]\n",
      "Accuracy of Tree 20 : 0.7803143093465674\n",
      "Classes:  [3 2 9 4 6]\n",
      "Accuracy of Tree 14 : 0.9223154362416107\n",
      "Classes:  [6 5 7 9 1]\n",
      "Accuracy of Tree 7 : 0.8757455268389662\n",
      "Classes:  [8 2 3 9 6]\n",
      "Accuracy of Tree 8 : 0.9103206412825652\n",
      "Classes:  [9 4 5 7 0]\n",
      "Accuracy of Tree 12 : 0.7776676577469441\n",
      "Classes:  [4 5 3 2 6]\n",
      "Accuracy of Tree 16 : 0.9474828566649941\n",
      "Classes:  [1 7 9 8 4]\n",
      "Accuracy of Tree 13 : 0.9477998996151916\n",
      "Classes:  [5 2 7 8 1]\n",
      "Accuracy of Tree 2 : 0.8694701429772919\n",
      "Classes:  [7 6 0 9 8]\n",
      "Accuracy of Tree 19 : 0.9556511666390866\n",
      "Classes:  [1 9 8 5 4]\n",
      "Accuracy of Tree 4 : 0.8385155466399198\n",
      "Classes:  [1 8 4 6 0]\n",
      "Accuracy of Tree 3 : 0.8871234690499834\n",
      "Classes:  [5 4 6 9 8]\n",
      "Accuracy of Tree 18 : 0.9124545754872811\n",
      "Classes:  [3 5 4 0 8]\n",
      "Accuracy of Tree 1 : 0.9337349397590361\n",
      "Classes:  [8 4 9 0 7]\n",
      "Accuracy of Tree 17 : 0.9327050264550265\n",
      "Classes:  [3 0 5 9 8]\n",
      "Accuracy of Tree 5 : 0.8588449445639583\n",
      "Classes:  [0 9 4 2 3]\n",
      "Accuracy of Tree 11 : 0.7352107982003\n",
      "Classes:  [8 6 2 4 0]\n",
      "Accuracy of Tree 32 : 0.8708493242115801\n",
      "Classes:  [1 6 3 0 9]\n",
      "Accuracy of Tree 38 : 0.9474391267842149\n",
      "Classes:  [1 0 9 7 4]\n",
      "Accuracy of Tree 34 : 0.8786520584329349\n",
      "Classes:  [1 6 3 2 5]\n",
      "Accuracy of Tree 40 : 0.8556355719309998\n",
      "Classes:  [5 7 3 0 6]\n",
      "Accuracy of Tree 30 : 0.9176626826029216\n",
      "Classes:  [1 4 0 3 9]\n",
      "Accuracy of Tree 24 : 0.9187800430963037\n",
      "Classes:  [4 3 1 5 0]\n",
      "Accuracy of Tree 31 : 0.8880721683929168\n",
      "Classes:  [7 1 5 4 2]\n",
      "Accuracy of Tree 39 : 0.8882597835137386\n",
      "Classes:  [9 1 0 2 4]\n",
      "Accuracy of Tree 27 : 0.9157981803143094\n",
      "Classes:  [9 8 4 3 0]\n",
      "Accuracy of Tree 37 : 0.8988223586000995\n",
      "Classes:  [9 2 1 8 4]\n",
      "Accuracy of Tree 35 : 0.8050125313283208\n",
      "Classes:  [2 7 5 6 4]\n",
      "Accuracy of Tree 36 : 0.9343823234014061\n",
      "Classes:  [8 7 2 5 0]\n",
      "Accuracy of Tree 26 : 0.9494729797557303\n",
      "Classes:  [7 1 5 2 8]\n",
      "Accuracy of Tree 25 : 0.7980914113510799\n",
      "Classes:  [4 8 6 7 2]\n",
      "Accuracy of Tree 28 : 0.7360439926678887\n",
      "Classes:  [0 6 2 8 4]\n",
      "Accuracy of Tree 22 : 0.9353866755942417\n",
      "Classes:  [2 8 0 7 5]\n",
      "Accuracy of Tree 21 : 0.9293747932517367\n",
      "Classes:  [5 2 0 9 3]\n",
      "Accuracy of Tree 23 : 0.8817222038931046\n",
      "Classes:  [9 4 8 2 5]\n",
      "Accuracy of Tree 29 : 0.9300826446280992\n",
      "Classes:  [9 8 5 3 6]\n",
      "Accuracy of Tree 33 : 0.7365439093484419\n",
      "Classes:  [6 8 4 0 2]\n",
      "Accuracy of Tree 43 : 0.8885344972301494\n",
      "Classes:  [6 1 7 4 9]\n",
      "Accuracy of Tree 47 : 0.923268147166059\n",
      "Classes:  [3 2 9 0 8]\n",
      "Accuracy of Tree 44 : 0.8603342710574218\n",
      "Classes:  [2 3 1 8 4]\n",
      "Accuracy of Tree 48 : 0.7028913260219342\n",
      "Classes:  [3 4 0 6 2]\n",
      "Accuracy of Tree 42 : 0.8597170764567195\n",
      "Classes:  [3 0 7 6 1]\n",
      "Accuracy of Tree 46 : 0.8735059760956175\n",
      "Classes:  [1 5 6 3 2]\n",
      "Accuracy of Tree 50 : 0.9028867011513433\n",
      "Classes:  [3 7 4 9 0]\n",
      "Accuracy of Tree 49 : 0.8426033538103935\n",
      "Classes:  [3 4 1 6 8]\n",
      "Accuracy of Tree 41 : 0.9162487462387161\n",
      "Classes:  [3 7 2 0 9]\n",
      "Accuracy of Tree 45 : 0.7340443259456757\n",
      "Classes:  [4 6 0 2 8]\n",
      "SLP input dimension: 250\n",
      "SLP Pred:  0.8589\n",
      "Prediction Tree Pred:  0.7722\n",
      "Majority Voting Pred:  0.8469\n"
     ]
    }
   ],
   "source": [
    "# Fashion MNIST\n",
    "print(\"Fashion MNIST\\n\")\n",
    "run_test(fm_X_train, fm_X_test, fm_y_train, fm_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifar\n",
      "\n",
      "Accuracy of Tree 7 : 0.37626112759643915\n",
      "Classes:  [9 5 6 3 2]\n",
      "Accuracy of Tree 15 : 0.40834664536741216\n",
      "Classes:  [1 4 5 7 9]\n",
      "Accuracy of Tree 4 : 0.4042211055276382\n",
      "Classes:  [2 8 4 0 7]\n",
      "Accuracy of Tree 16 : 0.41289163839552984\n",
      "Classes:  [6 1 5 9 2]\n",
      "Accuracy of Tree 20 : 0.3733174980205859\n",
      "Classes:  [4 7 5 9 3]\n",
      "Accuracy of Tree 3 : 0.44417910447761194\n",
      "Classes:  [4 7 9 3 8]\n",
      "Accuracy of Tree 11 : 0.4313765182186235\n",
      "Classes:  [4 1 0 3 6]\n",
      "Accuracy of Tree 12 : 0.39174641148325356\n",
      "Classes:  [4 3 0 7 5]\n",
      "Accuracy of Tree 8 : 0.43620414673046254\n",
      "Classes:  [0 2 8 9 4]\n",
      "Accuracy of Tree 9 : 0.3948004836759371\n",
      "Classes:  [4 2 1 9 6]\n",
      "Accuracy of Tree 19 : 0.455996807024546\n",
      "Classes:  [6 9 3 7 8]\n",
      "Accuracy of Tree 1 : 0.438802346752984\n",
      "Classes:  [6 0 8 4 3]\n",
      "Accuracy of Tree 2 : 0.32619672793375076\n",
      "Classes:  [2 5 6 7 4]\n",
      "Accuracy of Tree 10 : 0.43197616683217477\n",
      "Classes:  [1 7 0 3 5]\n",
      "Accuracy of Tree 6 : 0.41174121405750796\n",
      "Classes:  [9 4 1 7 5]\n",
      "Accuracy of Tree 13 : 0.3982089552238806\n",
      "Classes:  [5 2 4 9 1]\n",
      "Accuracy of Tree 5 : 0.40096230954290296\n",
      "Classes:  [2 3 1 4 8]\n",
      "Accuracy of Tree 17 : 0.3855755894590846\n",
      "Classes:  [3 2 5 7 1]\n",
      "Accuracy of Tree 14 : 0.3935613682092555\n",
      "Classes:  [3 5 4 0 6]\n",
      "Accuracy of Tree 18 : 0.42168674698795183\n",
      "Classes:  [4 9 6 0 2]\n",
      "Accuracy of Tree 28 : 0.3824284304047384\n",
      "Classes:  [0 7 5 2 3]\n",
      "Accuracy of Tree 23 : 0.41289163839552984\n",
      "Classes:  [9 1 5 6 2]\n",
      "Accuracy of Tree 40 : 0.3923999187157082\n",
      "Classes:  [4 2 6 7 1]\n",
      "Accuracy of Tree 35 : 0.4531437725270324\n",
      "Classes:  [5 7 1 0 8]\n",
      "Accuracy of Tree 31 : 0.41788650491277324\n",
      "Classes:  [5 8 7 4 2]\n",
      "Accuracy of Tree 27 : 0.4341443796501106\n",
      "Classes:  [7 8 5 6 2]\n",
      "Accuracy of Tree 32 : 0.43223880597014924\n",
      "Classes:  [0 8 3 1 5]\n",
      "Accuracy of Tree 24 : 0.4361595510122269\n",
      "Classes:  [3 8 4 0 7]\n",
      "Accuracy of Tree 39 : 0.45095693779904306\n",
      "Classes:  [8 9 6 3 0]\n",
      "Accuracy of Tree 21 : 0.42606266214328475\n",
      "Classes:  [7 8 9 2 4]\n",
      "Accuracy of Tree 36 : 0.46956872228939944\n",
      "Classes:  [8 3 1 0 6]\n",
      "Accuracy of Tree 38 : 0.31742243436754175\n",
      "Classes:  [3 7 5 2 4]\n",
      "Accuracy of Tree 25 : 0.45554665598718463\n",
      "Classes:  [1 7 0 5 8]\n",
      "Accuracy of Tree 34 : 0.42331658291457286\n",
      "Classes:  [8 7 5 4 0]\n",
      "Accuracy of Tree 22 : 0.4369205965336558\n",
      "Classes:  [1 0 8 2 4]\n",
      "Accuracy of Tree 30 : 0.43573604060913707\n",
      "Classes:  [4 8 6 1 3]\n",
      "Accuracy of Tree 26 : 0.4408121827411168\n",
      "Classes:  [1 6 4 8 3]\n",
      "Accuracy of Tree 37 : 0.4568741244746848\n",
      "Classes:  [6 8 9 7 5]\n",
      "Accuracy of Tree 29 : 0.3817527010804322\n",
      "Classes:  [3 7 1 4 5]\n",
      "Accuracy of Tree 33 : 0.46527078719549025\n",
      "Classes:  [7 6 1 9 8]\n",
      "Accuracy of Tree 43 : 0.38731965047754524\n",
      "Classes:  [6 7 1 4 2]\n",
      "Accuracy of Tree 44 : 0.4530441703143653\n",
      "Classes:  [7 6 3 9 0]\n",
      "Accuracy of Tree 47 : 0.42272449602932194\n",
      "Classes:  [1 8 4 2 6]\n",
      "Accuracy of Tree 48 : 0.4245452728362982\n",
      "Classes:  [0 1 5 4 3]\n",
      "Accuracy of Tree 41 : 0.40757818765036086\n",
      "Classes:  [0 7 6 2 5]\n",
      "Accuracy of Tree 50 : 0.4099839615076183\n",
      "Classes:  [6 2 0 7 5]\n",
      "Accuracy of Tree 42 : 0.43120539254559875\n",
      "Classes:  [3 9 8 1 7]\n",
      "Accuracy of Tree 49 : 0.4506009370543899\n",
      "Classes:  [4 1 6 0 7]\n",
      "Accuracy of Tree 45 : 0.47723169508525576\n",
      "Classes:  [8 0 6 9 7]\n",
      "Accuracy of Tree 46 : 0.43897559023609445\n",
      "Classes:  [9 3 6 8 1]\n",
      "SLP input dimension: 250\n",
      "SLP Pred:  0.4445\n",
      "Prediction Tree Pred:  0.2334\n",
      "Majority Voting Pred:  0.4368\n"
     ]
    }
   ],
   "source": [
    "# Cifar\n",
    "print(\"Cifar\\n\")\n",
    "run_test(cf_X_train, cf_X_test, cf_y_train, cf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(np.array([[1, 0], [1,1],[2,2],[1,3],[1,4],[1,5],[1,6],[1,7],[1,8],[1,9]]))\n",
    "one_hot_encoded_predictions = enc.transform(np.array([[1,1],[1,3]])).toarray() \n",
    "print(one_hot_encoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(10, size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(10*0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_label = np.random.choice(range(10), 3, replace=False)\n",
    "y_train_subset = y_train[y_train.label.isin(filter_label)]\n",
    "X_train_subset = X_train.loc[y_train_subset.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [i for i in range(60000)]\n",
    "slected_lst = np.random.choice(lst, 60000, replace=True)\n",
    "len(np.unique(slected_lst))/len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "slected_lst = []\n",
    "for i in range(60000):\n",
    "    rnd = np.random.randint(60000)\n",
    "    slected_lst.append(rnd)\n",
    "len(np.unique(slected_lst))/60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "newlist = sorted([{\"tree\": 4, \"accuracy\": 3, \"filter_label\": 2},{\"tree\": 2, \"accuracy\": 4, \"filter_label\": 5}], key=itemgetter('accuracy'), reverse=True)\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['a','b','c']\n",
    "accuracies = [0.98, 0.94,0.9]\n",
    "accuracies -= np.min(accuracies)\n",
    "print(accuracies)\n",
    "probabilities = np.asarray(accuracies)/sum(accuracies)\n",
    "print(probabilities)\n",
    "np.random.choice(lst, 1, p=probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_subset = y_train[y_train.label.isin([3,4])]\n",
    "X_train_subset = X_train.loc[y_train_subset.index]\n",
    "X_train_subtree = X_train_subset.sample(frac=1, replace=True, random_state=1)\n",
    "Y_train_subtree = y_train_subset.sample(frac=1, replace=True, random_state=1)\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "dtc = dtc.fit(X_train_subtree,Y_train_subtree)\n",
    "y_valid_filtered= y_test[y_test.label.isin(filter_label)]\n",
    "X_valid_filtered = X_test.loc[y_valid_filtered.index]\n",
    "y_pred = dtc.predict(X_valid_filtered)\n",
    "print(y_valid_filtered)\n",
    "print(y_pred)\n",
    "tree_accuracy = metrics.accuracy_score(y_valid_filtered, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,2],[1,4],[6,9]],columns=list('AB'))\n",
    "x = df.loc[:,'B'] > 0\n",
    "print(x)\n",
    "x = df['B'].isin([9,2])\n",
    "df[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
